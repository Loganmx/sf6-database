#!/usr/bin/env python3
"""
SF6 SuperCombo MediaWiki Scraper - Gerador de Manifest.json
============================================================
Scraper 100% gratuito para Street Fighter 6 assets da Wiki SuperCombo.
Usa MediaWiki API para evitar thumbnails e pegar URLs originais.

Estrutura de saÃ­da:
  assets/
    <personagem>/
      portrait/
      render/
    shared/
      input_icons/
  manifest.json

Autor: Gerado para projeto PT-BR zero-cost
"""

import requests
import json
import hashlib
import time
from pathlib import Path
from urllib.parse import urljoin, unquote
from typing import Dict, List, Optional

# ============================================================
# CONFIGURAÃ‡Ã•ES
# ============================================================
BASE_URL = "https://wiki.supercombo.gg"
API_URL = f"{BASE_URL}/api.php"
MAIN_PAGE = "Street_Fighter_6"
OUTPUT_DIR = Path("assets")
MANIFEST_PATH = Path("manifest.json")

# Rate limiting (respeitar o servidor)
REQUEST_DELAY = 0.5  # segundos entre requests

# PadrÃµes de nomenclatura (baseado nos exemplos do Ryu)
PORTRAIT_PATTERNS = ["Icon.png", "icon.png"]
RENDER_PATTERNS = ["Character_Select.png", "2C.png", "Portrait.png"]
INPUT_ICON_PATTERNS = [
    "SF6_Input_", "SF6_LP.png", "SF6_MP.png", "SF6_HP.png",
    "SF6_LK.png", "SF6_MK.png", "SF6_HK.png", "SF6_DR.png"
]

# ============================================================
# FUNÃ‡Ã•ES AUXILIARES
# ============================================================

def api_request(params: dict) -> dict:
    """Faz request na API MediaWiki com rate limiting"""
    time.sleep(REQUEST_DELAY)
    params["format"] = "json"
    try:
        response = requests.get(API_URL, params=params, timeout=15)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"âš ï¸  Erro na API: {e}")
        return {}

def download_image(url: str, dest_path: Path) -> bool:
    """Baixa imagem e salva no disco"""
    try:
        time.sleep(REQUEST_DELAY)
        response = requests.get(url, timeout=15, stream=True)
        response.raise_for_status()

        dest_path.parent.mkdir(parents=True, exist_ok=True)
        with open(dest_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        print(f"  âœ“ Baixado: {dest_path.name}")
        return True
    except Exception as e:
        print(f"  âœ— Erro ao baixar {url}: {e}")
        return False

def get_file_hash(filepath: Path) -> str:
    """Calcula hash SHA256 do arquivo"""
    sha256 = hashlib.sha256()
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            sha256.update(chunk)
    return sha256.hexdigest()

# ============================================================
# SCRAPER PRINCIPAL
# ============================================================

def get_character_list() -> List[str]:
    """Detecta todos os personagens da pÃ¡gina principal do SF6"""
    print("ğŸ” Detectando personagens...")

    data = api_request({
        "action": "parse",
        "page": MAIN_PAGE,
        "prop": "links",
        "redirects": 1
    })

    if "parse" not in data or "links" not in data["parse"]:
        print("âš ï¸  NÃ£o foi possÃ­vel detectar personagens")
        return []

    # Filtrar links que sÃ£o pÃ¡ginas de personagens
    # Geralmente seguem o padrÃ£o: Street_Fighter_6/Nome
    characters = []
    for link in data["parse"]["links"]:
        title = link.get("*", "")
        if title.startswith("Street_Fighter_6/") and "/" in title:
            char_name = title.split("/")[-1]
            # Ignorar pÃ¡ginas de sistema/meta
            if char_name and not any(x in char_name.lower() for x in ["frame", "data", "system", "mechanics"]):
                characters.append(char_name)

    print(f"âœ“ Encontrados {len(characters)} personagens")
    return sorted(set(characters))

def get_page_images(page_title: str) -> List[Dict]:
    """Lista todas as imagens de uma pÃ¡gina com URLs originais"""
    data = api_request({
        "action": "query",
        "titles": page_title,
        "prop": "images",
        "imlimit": "max"
    })

    if "query" not in data or "pages" not in data["query"]:
        return []

    page = list(data["query"]["pages"].values())[0]
    if "images" not in page:
        return []

    image_titles = [img["title"] for img in page["images"]]

    # Buscar URLs originais das imagens
    images_data = []
    for i in range(0, len(image_titles), 50):  # API limita a 50 por request
        batch = image_titles[i:i+50]
        img_data = api_request({
            "action": "query",
            "titles": "|".join(batch),
            "prop": "imageinfo",
            "iiprop": "url|size|mime"
        })

        if "query" in img_data and "pages" in img_data["query"]:
            for page_data in img_data["query"]["pages"].values():
                if "imageinfo" in page_data:
                    info = page_data["imageinfo"][0]
                    images_data.append({
                        "title": page_data["title"].replace("File:", ""),
                        "url": info["url"],
                        "size": info.get("size", 0),
                        "mime": info.get("mime", "")
                    })

    return images_data

def find_best_match(images: List[Dict], patterns: List[str], char_name: str) -> Optional[Dict]:
    """Encontra a melhor imagem baseada nos padrÃµes"""
    for pattern in patterns:
        for img in images:
            title = img["title"]
            # Verifica se o nome do personagem e o padrÃ£o estÃ£o no tÃ­tulo
            if char_name.replace(" ", "_") in title and pattern in title:
                return img
    return None

def scrape_character(char_name: str) -> Dict:
    """Scrape completo de um personagem"""
    print(f"\nğŸ“¦ Processando: {char_name}")

    page_title = f"Street_Fighter_6/{char_name}"
    images = get_page_images(page_title)

    if not images:
        print(f"  âš ï¸  Nenhuma imagem encontrada")
        return {"name": char_name, "assets": {}}

    print(f"  â†’ {len(images)} imagens encontradas na pÃ¡gina")

    result = {
        "name": char_name,
        "wiki_url": f"{BASE_URL}/w/{page_title.replace(' ', '_')}",
        "assets": {}
    }

    # Buscar Portrait
    portrait = find_best_match(images, PORTRAIT_PATTERNS, char_name)
    if portrait:
        dest = OUTPUT_DIR / char_name / "portrait" / portrait["title"]
        if download_image(portrait["url"], dest):
            result["assets"]["portrait"] = f"/assets/{char_name}/portrait/{portrait['title']}"

    # Buscar Render (com fallback)
    render = find_best_match(images, RENDER_PATTERNS, char_name)
    if render:
        dest = OUTPUT_DIR / char_name / "render" / render["title"]
        if download_image(render["url"], dest):
            result["assets"]["render"] = f"/assets/{char_name}/render/{render['title']}"

    return result

def scrape_input_icons() -> List[Dict]:
    """Scrape dos Ã­cones de input (comandos) compartilhados"""
    print("\nğŸ® Processando Ã­cones de input...")

    # Buscar na pÃ¡gina principal ou em uma pÃ¡gina de notaÃ§Ã£o
    images = get_page_images("Street_Fighter_6")

    input_icons = []
    seen_hashes = set()

    for img in images:
        title = img["title"]
        # Filtrar apenas Ã­cones de input
        if any(pattern in title for pattern in INPUT_ICON_PATTERNS):
            dest = OUTPUT_DIR / "shared" / "input_icons" / title
            if download_image(img["url"], dest):
                file_hash = get_file_hash(dest)

                # Evitar duplicatas
                if file_hash not in seen_hashes:
                    seen_hashes.add(file_hash)
                    input_icons.append({
                        "name": title.replace("SF6_", "").replace(".png", ""),
                        "path": f"/assets/shared/input_icons/{title}",
                        "hash": file_hash
                    })

    print(f"âœ“ {len(input_icons)} Ã­cones Ãºnicos baixados")
    return input_icons

# ============================================================
# MAIN
# ============================================================

def main():
    print("=" * 60)
    print("SF6 SUPERCOMBO SCRAPER - GERADOR DE MANIFEST")
    print("=" * 60)

    # Criar estrutura de pastas
    OUTPUT_DIR.mkdir(exist_ok=True)

    # 1. Detectar personagens
    characters = get_character_list()

    if not characters:
        print("âŒ Nenhum personagem detectado. Verifique a conexÃ£o.")
        return

    # 2. Scrape de cada personagem
    character_data = []
    for char in characters:
        data = scrape_character(char)
        if data["assets"]:  # SÃ³ adiciona se tiver pelo menos 1 asset
            character_data.append(data)

    # 3. Scrape dos Ã­cones de input
    input_icons = scrape_input_icons()

    # 4. Gerar manifest.json
    manifest = {
        "version": "1.0.0",
        "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "source": "SuperCombo Wiki (MediaWiki API)",
        "license": "Fair Use - Informational Purpose",
        "characters": character_data,
        "shared_assets": {
            "input_icons": input_icons
        },
        "stats": {
            "total_characters": len(character_data),
            "total_input_icons": len(input_icons)
        }
    }

    with open(MANIFEST_PATH, 'w', encoding='utf-8') as f:
        json.dump(manifest, f, indent=2, ensure_ascii=False)

    print("\n" + "=" * 60)
    print("âœ… SCRAPING CONCLUÃDO!")
    print("=" * 60)
    print(f"ğŸ“Š Personagens processados: {len(character_data)}")
    print(f"ğŸ® Ãcones de input: {len(input_icons)}")
    print(f"ğŸ“„ Manifest gerado: {MANIFEST_PATH}")
    print(f"ğŸ“ Assets salvos em: {OUTPUT_DIR}/")
    print("\nğŸ’¡ PrÃ³ximos passos:")
    print("   1. Copie a pasta 'assets/' para 'public/assets/' do seu React")
    print("   2. Copie 'manifest.json' para 'public/manifest.json'")
    print("   3. Rode 'npm run build' e suba para o GitHub")
    print("=" * 60)

if __name__ == "__main__":
    main()
